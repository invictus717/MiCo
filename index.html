<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-M6F3SVYN3B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-M6F3SVYN3B');
  </script>
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Explore the Limits of Omni-modal Pretraining at Scale</title>
  <link rel="icon" type="image/x-icon" href="static/images/cuhk.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">Explore the Limits of Omni-modal Pretraining at Scale</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://invictus717.github.io/" target="_blank">Yiyuan Zhang</a><sup>1,4*</sup>,</span>
                <span class="author-block">
                  <a href="https://openreview.net/profile?id=~Handong_Li/" target="_blank">Handong Li</a><sup>2,3*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ" target="_blank">Jing Liu</a><sup>1†</sup>,</span>
                  <span class="author-block">
                    <a href="https://xyue.io/" target="_blank">Xiangyu Yue</a><sup>1</sup>
                  </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Multimedia Lab, The Chinese University of Hong Kong   
                      <br> <sup>2</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences</span>
                      <br> <sup>3</sup> Institute of Automation, Chinese Academy of Science</span>
                      <sup>4</sup> Shanghai AI Laboratory</span>
                    </div>
                  
                  <div class="column has-text-centered">
                    <div class="publication-links">
                    <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2401.14405" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>

                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.14405.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/invictus717/MiCo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                 <!-- Hugging Face.-->
                <span class="link-block">
                <a href="https://huggingface.co/Yiyuan/MiCo-ViT-g-14-omnimodal-300k-b64K" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      &#129303;
                  </span>
                  <span>Hugging Face Models</span>
                </a>
              </span> 
              <span class="link-block">
                <a href="https://huggingface.co/Yiyuan/MiCo-ViT-g-14-omnimodal-300k-b64K" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      &#129303;
                  </span>
                  <span>HF Dataset (TODO)</span>
                </a>
              </span> 
                </a>
              </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<video poster="" id="tree" autoplay controls muted loop height="100%">

  <source src="static/videos/banner_video.mp4"
  type="video/mp4">
</video>
-->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img  src="static/images/omnimodal_pretraining.png" alt="Teaser"/>
      <h2 class="subtitle has-text-centered">
        <span></span> <b>Omni-modal Pretraining.</b> We propose collecting large-scale omni-modal paired data, including text,
        image, video, depth, and normal maps, to learn universal representations. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">How the human brain performs coherent multimodal cognition?</h2>
        <div class="content has-text-justified">
          <img  width="1450px" src="static/images/brain.png" alt="idea"/>
          <p>
            As outlined in Richard Mayer's Cognitive Theory of Multimedia Learning,our brain processes multimedia signals through two distinct channels—auditory and visual—in sensory memory, as depicted in Figure(a). The sensory memory integrates these signals with prior knowledge through words, transforming new multimedia information into long-term memory. Notably, 1) multimedia signals in the brain share channels, and 2) words function as the reasoning interface in our brain.
            <br>
            <b>Inspired by these insights, we categorize diverse modalities into two types</b>: knowledge modality and interface modality. Knowledge modalities, primarily derived from raw sensors, contribute knowledge in diverse formats. For example, images and depth maps offer visual knowledge, while audio and video provide auditory and spatiotemporal knowledge. The language modality, developed by humans, is inherently more abstract and naturally functions as the interface modality, facilitating learning, reasoning, and the coordination of knowledge. To this end, we design an omni-modal learning architecture, illustrated in Figure (b), with two distinct branches: one for knowledge modalities and one for the interface modality, i.e. natural language. The knowledge and interface modalities are aligned through a novel generative reasoning method.
          </p>
        </div>

        <h2 class="title is-3">Our Contributions</h2>
        <div class="content has-text-justified">
          <p>
            <h6 class="title is-6">1. We collect an omni-modal dataset including comprehensive paired video, audio, visual caption, audio caption, estimated depth and normal maps. And we are working on preparing more paired modalitie.</h5>
            <h6 class="title is-6">2. We propose a scalable pretraining paradigm, Multimodal Context (MiCo), which enpowers models learn universal representations across any modalities.</h5>
            <h6 class="title is-6">3. We conduct extensive experiments on 10 single-modality tasks, 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and 18 MLLM benchmarks. Our models achieve 37
              new state-of-the-art performances.</h6>
            <h6 class="title is-6"> In brief, it's the beginning to explore <b>multimodal scaling laws</b>, use MiCo-ViT with LLMs, outperforms LLaVA-1.5 in vision and almost any MLLMs in video, audio, and 3D zero-shot QA tasks. </h6>
            </p>
            <img  width="1450px" src="static/images/scaling_law.png" alt="idea"/>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="95%" SIZE=5>
</div>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column ">
        <h2 class="title is-3">Evolution of Pretraining Paradigms </h2>
          <br>
        <div class="center-div">
          <img width="850px" src="static/images/paradigm.png" alt="MY ALT TEXT"/>
        </div>   
        <p>Masked modeling has shown great success in singlemodality
          general-purpose understanding. Contrastive learning distinguishes transferable features
          with modality tuples. 
          <b>We aim to achieve general-purpose omni-modal understanding and learn transferable,
            universal representations.</b> </p>
      </div>
    </div>
  </div>
</section>

<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>


<!-- Video end
<div class="center-div">
</div>
<br>
<table align="center" width="600px">
  <tbody>
      <tr>
          <td>
              <div class="center-div">
                <iframe width="750px" height="415" src="https://www.youtube.com/embed/V8L8xbsTyls" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </div>               
          </td>
      </tr>
      <tr>
  </tbody>
</table> -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method: Multimodal Context</h2>
        <div class="content has-text-justified">
          <img src="static/images/framework.png" alt="MY ALT TEXT"/>
          <p>
            <b>Overview of Multimodal Context Pretraining Paradigm</b> We use a shared ViT for multimodal
feature extraction, and another branch is to employ a text encoder. We concatenate these multimodal sequences
as multimodal contexts and perform contrastive learning and masked modeling.
          </p>
          <h4>Multimodal Context Construction (See paper for exact details)</h4>
          <p>As mentioned in § 3.1, we build a dataset with multimodal paired data {(x<sub>I</sub>, x<sub>D</sub>, x<sub>N</sub>, x<sub>T</sub><sup>I</sup>), (x<sub>A</sub>, x<sub>T</sub><sup>A</sup>), (x<sub>V</sub>, x<sub>T</sub><sup>V</sup>)}, then we employ the omni-modal encoder f(&#x2022;; θ) to extract features z<sub>I</sub>, z<sub>A</sub>, z<sub>V</sub>, z<sub>D</sub>, and z<sub>N</sub>, then use the text encoder to extract text features z<sub>T</sub>. Therefore, we construct the context by a top-down design:</p>
          <ol>
              <li>For the whole multimodal embeddings, they share the same position embeddings E<sub>Pos</sub> to build a modality-fused context relationship across diverse modalities.</li>
              <li>Then, for each specific context, they're labeled by modality embeddings including E<sub>M</sub><sup>I</sup>, E<sub>M</sub><sup>A</sup>, E<sub>M</sub><sup>V</sup>, E<sub>M</sub><sup>D</sup>, E<sub>M</sub><sup>N</sup>, etc., to indicate modality types.</li>
              <li>Within the same modality context, we employ the context embeddings E<sub>C</sub><sup>I</sup> to construct uni-modal context relationships.</li>
          </ol>
          <p>Thus, the construction of the multimodal context can be formulated as:</p>
          <div class="equation">
              <p>z<sub>I</sub> = [z<sub>I</sub><sup>1</sup>, z<sub>I</sub><sup>2</sup>, ..., z<sub>I</sub><sup>L<sub>I</sub></sup>] + E<sub>C</sub><sup>I</sup>, &nbsp;&nbsp;&nbsp;&nbsp; for each modality,</p>
              <p>z = [z<sub>I</sub> + E<sub>M</sub><sup>I</sup>, z<sub>A</sub> + E<sub>M</sub><sup>A</sup>, z<sub>V</sub> + E<sub>M</sub><sup>V</sup>, z<sub>D</sub> + E<sub>M</sub><sup>D</sup>, z<sub>N</sub> + E<sub>M</sub><sup>N</sup>] + E<sub>Pos</sub>,</p>
          </div>
          <p>where E<sub>C</sub><sup>I</sup> is up to the sample length of a specific modality. Meanwhile, the text features of specific captions can be easily concatenated, where their position embeddings E<sub>Pos</sub><sup>&#x2032;</sup> are also shared:</p>
          <div class="equation">
              <p>z<sub>T</sub> = [z<sub>T</sub><sup>I</sup>, z<sub>T</sub><sup>A</sup>, z<sub>T</sub><sup>V</sup>] + E<sub>Pos</sub><sup>&#x2032;</sup>.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>


<!-- Result -->
<!-- <section class="section hero is-light"> -->
<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our model on three different benchmarks: 1) Single-modality Understanding (following previous practices in fine-tuning & zero-shot setting in classification and
            forecasting tasks), 2) Cross-modality Understanding (following BEiT-3, VAST in finetuning
            and dataset splits for Caption, QA, and retrieval tasks), and 3) Multimodal Understanding with Large Language Models (following LLava, VideoChat, and OneLLM in multimodal
            zero-shot QA)
          </p>
          <div style="text-align:center;">
            <img width="850%" src="static/images/single.png" alt="exp-singel-modality"/>
          </div>
          <div style="text-align:center;">
            <img width="850%" src="static/images/cross.png" alt="exp-singel-modality"/>
          </div>
          <div style="text-align:center;">
            <img width="850%" src="static/images/vllm.png" alt="exp-singel-modality"/>
          </div>
          <div style="text-align:center;">
            <img width="850%" src="static/images/mllm.png" alt="exp-singel-modality"/>
          </div>
          <div style="text-align:center;">
            <img width="850%" src="static/images/videollm.png" alt="exp-singel-modality"/>
          </div>
          <h6 class="title is-6">Extensive experiments on 10 single-modality tasks, 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and 18 MLLM benchmarks present the superiority of MiCo. Our models achieve 37
            new state-of-the-art performances.</h6>
        
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      If you find our work useful, please cite our paper. BibTex code is provided below:
      <pre><code>@article{zhang2024explore,
        title={Explore the Limits of Omni-modal Pretraining at Scale},
        author={Zhang, Yiyuan and Li, Handong and Liu, Jing and Yue, Xiangyu},
        journal={arXiv preprint arXiv:2406.xxxxx},
        year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
